{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COMS4995 HW1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "c2S_wf_djoMA"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/zhichengMLE/deep_learning/blob/master/COMS4995_HW1.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "17vt1UKtbiHg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f7KQxHltGIOE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a5cbe0fb-b191-4484-dfc5-0eb5b91dd507"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch, torchvision\n",
        "import torch.optim as optim\n",
        "from numpy import float32\n",
        "import random\n",
        "\n",
        "torch.__version__"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.3.0.post4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "iwbCLyPe87OH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "24c9b733-26bd-46c4-fcf8-f9f433b4a2eb"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=100, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FvhqX2vDkFpC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 1"
      ]
    },
    {
      "metadata": {
        "id": "e0Rm0BNcXCW6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset_size = len(trainset)\n",
        "train_ratio = 0.9\n",
        "image_flatten_size = trainset[0][0].numpy().size\n",
        "train_size = int(train_ratio * dataset_size)\n",
        "valid_size = dataset_size - train_size\n",
        "test_size = len(testset)\n",
        "\n",
        "\n",
        "X_train = np.zeros((image_flatten_size, train_size), dtype=float32)\n",
        "X_val = np.zeros((image_flatten_size, valid_size), dtype=float32)\n",
        "X_test = np.zeros((image_flatten_size, test_size), dtype=float32)\n",
        "y_train  = np.zeros((train_size), dtype=int)\n",
        "y_val  = np.zeros((valid_size), dtype=int)\n",
        "y_test  = np.zeros((test_size), dtype=int)\n",
        "\n",
        "for i in range(train_size):\n",
        "    X_train[:,i] = trainset[i][0].numpy().flatten()\n",
        "    y_train[i] = trainset[i][1] \n",
        "for i in range(valid_size):\n",
        "    X_val[:,i] = trainset[i+train_size][0].numpy().flatten()\n",
        "    y_val[i] = trainset[i+train_size][1] \n",
        "for i in range(test_size):\n",
        "    X_test[:,i] = testset[i][0].numpy().flatten()\n",
        "    y_test[i] = trainset[i][1] \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nCrEQWvwGi65",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(object):\n",
        "    def __init__(self, layer_dimensions):\n",
        "        np.random.seed(100)\n",
        "        self.nb_layers = len(layer_dimensions)\n",
        "        \n",
        "        self.parameters = {}\n",
        "        self.parameters['W'] = {}\n",
        "        self.parameters['b'] = {}\n",
        "        self.parameters['Z'] = {}\n",
        "        self.parameters['A'] = {}\n",
        "        self.parameters['m'] = {}\n",
        "        self.parameters['mb'] = {}\n",
        "        self.parameters['s'] = {}\n",
        "        self.parameters['sb'] = {}\n",
        "        for i in range(self.nb_layers):\n",
        "            self.parameters['W'][i] = np.random.randn(layer_dimensions[i], layer_dimensions[i - 1]) / np.sqrt(layer_dimensions[i - 1])\n",
        "            self.parameters['b'][i] = np.zeros([layer_dimensions[i], 1])\n",
        "            self.parameters['m'][i] = None\n",
        "            self.parameters['mb'][i] = None\n",
        "            self.parameters['s'][i] = None\n",
        "            self.parameters['sb'][i] = None\n",
        "\n",
        "    def affineForward(self, A, W, b):\n",
        "        Z = np.dot(W, A) + b\n",
        "        return Z\n",
        "        \n",
        "    def activationForward(self, A, activation=\"relu\"):\n",
        "        if activation == \"relu\":\n",
        "            return self.relu(A)\n",
        "        elif activation == \"softmax\":\n",
        "            return self.softmax(A)\n",
        "        else:\n",
        "            raise NotImplemented\n",
        "            \n",
        "    def forwardPropagation(self, X):\n",
        "        self.parameters[\"A\"][0] = X\n",
        "        for i in range(1, self.nb_layers-1):\n",
        "            Z = self.affineForward(self.parameters[\"A\"][i-1], self.parameters[\"W\"][i], self.parameters[\"b\"][i])\n",
        "            A = self.activationForward(Z)\n",
        "            self.parameters[\"Z\"][i] = Z\n",
        "            self.parameters[\"A\"][i] = A\n",
        "        \n",
        "        # Softmax for last layer.\n",
        "        Z = self.affineForward(self.parameters[\"A\"][self.nb_layers-2], self.parameters[\"W\"][self.nb_layers-1], self.parameters[\"b\"][self.nb_layers-1])\n",
        "        AL = self.activationForward(Z, \"softmax\")\n",
        "        self.parameters[\"Z\"][self.nb_layers-1] = Z\n",
        "        self.parameters[\"A\"][self.nb_layers-1] = AL\n",
        "            \n",
        "        return AL\n",
        "    \n",
        "    def costFunction(self, AL, y):\n",
        "        cost = -np.sum(np.log(AL[y,range(y.shape[0])])) / y.shape[0]\n",
        "        AL[y, range(y.shape[0])] -= 1\n",
        "        dAL = AL / y.shape[0]\n",
        "        return cost, dAL\n",
        "\n",
        "    def affineBackward(self, dA_prev, cache, idx_layer=0):\n",
        "        W, Z, A = cache\n",
        "        mW, mb, s, sb = self.parameters['m'][idx_layer],self.parameters['mb'][idx_layer],self.parameters['s'][idx_layer],self.parameters['sb'][idx_layer]\n",
        "\n",
        "        dA = self.activationBackward(np.dot(W.T, dA_prev), Z)\n",
        "        dW = np.dot(dA_prev, A.T)\n",
        "        db = np.expand_dims(np.sum(dA_prev, axis=1), axis=1)            \n",
        "        \n",
        "        return dA, dW, db\n",
        "\n",
        "    def activationBackward(self, dA, cache):\n",
        "        return dA * self.relu_derivative(cache)\n",
        "    \n",
        "    def backPropagation(self, dAL, Y, cache):\n",
        "        dA_prev, dW, db = self.affineBackward(dAL, (self.parameters['W'][self.nb_layers-1], self.parameters['Z'][self.nb_layers-2], self.parameters['A'][self.nb_layers-2]), self.nb_layers-1)\n",
        "        gradients = {}\n",
        "        gradients[self.nb_layers-1] = {}\n",
        "        gradients[self.nb_layers-1]['dW'] = dW\n",
        "        gradients[self.nb_layers-1]['db'] = db\n",
        "        \n",
        "        for i in range(self.nb_layers-3, -1, -1):\n",
        "            dA_prev,dW,db = self.affineBackward(dA_prev, (self.parameters['W'][i+1], self.parameters['Z'][i], self.parameters['A'][i]), i+1)\n",
        "            gradients[i+1] = {}\n",
        "            gradients[i+1]['dW'] = dW\n",
        "            gradients[i+1]['db'] = db\n",
        "            \n",
        "        return gradients\n",
        "\n",
        "    def updateParameters(self, gradients, alpha):\n",
        "        for i in range(1, self.nb_layers):\n",
        "            self.parameters['W'][i] = self.parameters['W'][i] - alpha * gradients[i]['dW']\n",
        "            self.parameters['b'][i] = self.parameters['b'][i] - alpha * gradients[i]['db']\n",
        "\n",
        "    def train(self, X_train, X_val, y_train, y_val, iters, alpha, batch_size):\n",
        "        print_batch_freq = 30\n",
        "        print_epoch_freq = 1\n",
        "        alpha_decay_rate = 0.99\n",
        "        min_alpha = 0.0001\n",
        "        X_train_orig = X_train\n",
        "        y_train_orig = y_train\n",
        "        \n",
        "        for i in range(0, iters):\n",
        "            # shuffle all trainning data.\n",
        "            index = np.random.randint(0, X_train_orig.shape[1], size=X_train_orig.shape[1])\n",
        "            X_train = X_train_orig[:, index]\n",
        "            y_train = y_train_orig[index]\n",
        "\n",
        "            for j in range(math.ceil(X_train.shape[1] // batch_size)):\n",
        "                # minibatch\n",
        "                X_batch, y_batch = self.get_batch(X_train, y_train, j*batch_size, batch_size)\n",
        "                # forward propogation\n",
        "                AL = self.forwardPropagation(X_batch)\n",
        "                # compute loss function\n",
        "                cost, dAL = self.costFunction(AL, y_batch)\n",
        "                # compute gradients of parameters\n",
        "                self.parameters['Z'][0] = X_batch\n",
        "                gradients = self.backPropagation(dAL, y_batch, None)\n",
        "                # update parameters\n",
        "                self.updateParameters(gradients, alpha)\n",
        "                \n",
        "                if(i % print_epoch_freq == 0 and (j + 1) % print_batch_freq == 0):\n",
        "                    acc = np.sum(self.predict(X_batch) == y_batch) / y_batch.shape[0]\n",
        "                    validation_acc = np.sum(self.predict(X_val) == y_val) / y_val.shape[0]\n",
        "                    print(\"Iteration: %.5d/%.5d - Batch: [%.5d~%.5d]/%.5d - Cost: %.8f - Accuracy: %.8f - Validation Accuracy: %.8f - alpha: %.8f\"%(i, iters, j*batch_size, (j+1)*batch_size, X_train.shape[1], cost, acc, validation_acc, alpha))\n",
        "            if(alpha > min_alpha):\n",
        "                alpha *= alpha_decay_rate\n",
        "\n",
        "    def predict(self, X_new):\n",
        "        A_prev = X_new\n",
        "        for i in range(1, self.nb_layers):\n",
        "            A = A_prev\n",
        "            Z = self.affineForward(A, self.parameters[\"W\"][i], self.parameters[\"b\"][i])\n",
        "            A_prev = self.activationForward(Z)\n",
        "        y_pred = np.argmax(A_prev, axis=0)\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "    def get_batch(self, X, y, idx, batch_size):\n",
        "        index = np.random.randint(0,X.shape[1],size=batch_size)\n",
        "        return X[:,index], y[index]\n",
        "  \n",
        "    def relu(self, X):\n",
        "        return np.maximum(0, X)\n",
        "    \n",
        "    def softmax(self, X):\n",
        "        return np.exp(X) / np.sum(np.exp(X), axis = 0)  \n",
        "\n",
        "    def relu_derivative(self, A):\n",
        "        return np.where(A > 0, 1, 0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eQC9xpyxKIUE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "d90cb82b-8ec1-42df-c618-baf85b61c754"
      },
      "cell_type": "code",
      "source": [
        "layer_dimensions = [X_train.shape[0], 512, 256, 128, 64, 32, 10]\n",
        "NN = NeuralNetwork(layer_dimensions)\n",
        "NN.train(X_train, X_val, y_train, y_val, iters=100, alpha=0.05, batch_size=500)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 00000/00100 - Batch: [14500~15000]/45000 - Cost: 2.24466163 - Accuracy: 0.18800000 - Validation Accuracy: 0.19600000 - alpha: 0.05000000\n",
            "Iteration: 00000/00100 - Batch: [29500~30000]/45000 - Cost: 2.10547527 - Accuracy: 0.25600000 - Validation Accuracy: 0.26700000 - alpha: 0.05000000\n",
            "Iteration: 00000/00100 - Batch: [44500~45000]/45000 - Cost: 1.99304939 - Accuracy: 0.31000000 - Validation Accuracy: 0.28360000 - alpha: 0.05000000\n",
            "Iteration: 00001/00100 - Batch: [14500~15000]/45000 - Cost: 1.94042853 - Accuracy: 0.32000000 - Validation Accuracy: 0.32360000 - alpha: 0.04950000\n",
            "Iteration: 00001/00100 - Batch: [29500~30000]/45000 - Cost: 1.77014902 - Accuracy: 0.37200000 - Validation Accuracy: 0.34460000 - alpha: 0.04950000\n",
            "Iteration: 00001/00100 - Batch: [44500~45000]/45000 - Cost: 1.85142353 - Accuracy: 0.36000000 - Validation Accuracy: 0.35980000 - alpha: 0.04950000\n",
            "Iteration: 00002/00100 - Batch: [14500~15000]/45000 - Cost: 1.64861314 - Accuracy: 0.44200000 - Validation Accuracy: 0.38080000 - alpha: 0.04900500\n",
            "Iteration: 00002/00100 - Batch: [29500~30000]/45000 - Cost: 1.71036438 - Accuracy: 0.39400000 - Validation Accuracy: 0.38720000 - alpha: 0.04900500\n",
            "Iteration: 00002/00100 - Batch: [44500~45000]/45000 - Cost: 1.65072561 - Accuracy: 0.44800000 - Validation Accuracy: 0.39880000 - alpha: 0.04900500\n",
            "Iteration: 00003/00100 - Batch: [14500~15000]/45000 - Cost: 1.62870709 - Accuracy: 0.42400000 - Validation Accuracy: 0.40780000 - alpha: 0.04851495\n",
            "Iteration: 00003/00100 - Batch: [29500~30000]/45000 - Cost: 1.58815809 - Accuracy: 0.46400000 - Validation Accuracy: 0.41740000 - alpha: 0.04851495\n",
            "Iteration: 00003/00100 - Batch: [44500~45000]/45000 - Cost: 1.54890083 - Accuracy: 0.49800000 - Validation Accuracy: 0.42200000 - alpha: 0.04851495\n",
            "Iteration: 00004/00100 - Batch: [14500~15000]/45000 - Cost: 1.64388705 - Accuracy: 0.43800000 - Validation Accuracy: 0.42160000 - alpha: 0.04802980\n",
            "Iteration: 00004/00100 - Batch: [29500~30000]/45000 - Cost: 1.55825555 - Accuracy: 0.46800000 - Validation Accuracy: 0.44700000 - alpha: 0.04802980\n",
            "Iteration: 00004/00100 - Batch: [44500~45000]/45000 - Cost: 1.45790778 - Accuracy: 0.52400000 - Validation Accuracy: 0.44960000 - alpha: 0.04802980\n",
            "Iteration: 00005/00100 - Batch: [14500~15000]/45000 - Cost: 1.49414877 - Accuracy: 0.52600000 - Validation Accuracy: 0.45920000 - alpha: 0.04754950\n",
            "Iteration: 00005/00100 - Batch: [29500~30000]/45000 - Cost: 1.50547523 - Accuracy: 0.48400000 - Validation Accuracy: 0.45240000 - alpha: 0.04754950\n",
            "Iteration: 00005/00100 - Batch: [44500~45000]/45000 - Cost: 1.44493958 - Accuracy: 0.54800000 - Validation Accuracy: 0.46760000 - alpha: 0.04754950\n",
            "Iteration: 00006/00100 - Batch: [14500~15000]/45000 - Cost: 1.35687082 - Accuracy: 0.53200000 - Validation Accuracy: 0.47480000 - alpha: 0.04707401\n",
            "Iteration: 00006/00100 - Batch: [29500~30000]/45000 - Cost: 1.33541466 - Accuracy: 0.54200000 - Validation Accuracy: 0.46340000 - alpha: 0.04707401\n",
            "Iteration: 00006/00100 - Batch: [44500~45000]/45000 - Cost: 1.42458513 - Accuracy: 0.52800000 - Validation Accuracy: 0.48340000 - alpha: 0.04707401\n",
            "Iteration: 00007/00100 - Batch: [14500~15000]/45000 - Cost: 1.41188667 - Accuracy: 0.52600000 - Validation Accuracy: 0.46500000 - alpha: 0.04660327\n",
            "Iteration: 00007/00100 - Batch: [29500~30000]/45000 - Cost: 1.43308670 - Accuracy: 0.54400000 - Validation Accuracy: 0.48420000 - alpha: 0.04660327\n",
            "Iteration: 00007/00100 - Batch: [44500~45000]/45000 - Cost: 1.32215426 - Accuracy: 0.56400000 - Validation Accuracy: 0.45920000 - alpha: 0.04660327\n",
            "Iteration: 00008/00100 - Batch: [14500~15000]/45000 - Cost: 1.37073714 - Accuracy: 0.55200000 - Validation Accuracy: 0.47380000 - alpha: 0.04613723\n",
            "Iteration: 00008/00100 - Batch: [29500~30000]/45000 - Cost: 1.25285926 - Accuracy: 0.61800000 - Validation Accuracy: 0.49680000 - alpha: 0.04613723\n",
            "Iteration: 00008/00100 - Batch: [44500~45000]/45000 - Cost: 1.30080465 - Accuracy: 0.59800000 - Validation Accuracy: 0.50240000 - alpha: 0.04613723\n",
            "Iteration: 00009/00100 - Batch: [14500~15000]/45000 - Cost: 1.18983648 - Accuracy: 0.65200000 - Validation Accuracy: 0.49540000 - alpha: 0.04567586\n",
            "Iteration: 00009/00100 - Batch: [29500~30000]/45000 - Cost: 1.30844889 - Accuracy: 0.60000000 - Validation Accuracy: 0.49300000 - alpha: 0.04567586\n",
            "Iteration: 00009/00100 - Batch: [44500~45000]/45000 - Cost: 1.27873517 - Accuracy: 0.58000000 - Validation Accuracy: 0.48440000 - alpha: 0.04567586\n",
            "Iteration: 00010/00100 - Batch: [14500~15000]/45000 - Cost: 1.22573668 - Accuracy: 0.62200000 - Validation Accuracy: 0.50620000 - alpha: 0.04521910\n",
            "Iteration: 00010/00100 - Batch: [29500~30000]/45000 - Cost: 1.32582802 - Accuracy: 0.59400000 - Validation Accuracy: 0.48020000 - alpha: 0.04521910\n",
            "Iteration: 00010/00100 - Batch: [44500~45000]/45000 - Cost: 1.25948463 - Accuracy: 0.60800000 - Validation Accuracy: 0.49560000 - alpha: 0.04521910\n",
            "Iteration: 00011/00100 - Batch: [14500~15000]/45000 - Cost: 1.25477425 - Accuracy: 0.62200000 - Validation Accuracy: 0.49520000 - alpha: 0.04476691\n",
            "Iteration: 00011/00100 - Batch: [29500~30000]/45000 - Cost: 1.16900375 - Accuracy: 0.64400000 - Validation Accuracy: 0.50580000 - alpha: 0.04476691\n",
            "Iteration: 00011/00100 - Batch: [44500~45000]/45000 - Cost: 1.23647436 - Accuracy: 0.62600000 - Validation Accuracy: 0.49560000 - alpha: 0.04476691\n",
            "Iteration: 00012/00100 - Batch: [14500~15000]/45000 - Cost: 1.19153001 - Accuracy: 0.62000000 - Validation Accuracy: 0.51700000 - alpha: 0.04431924\n",
            "Iteration: 00012/00100 - Batch: [29500~30000]/45000 - Cost: 1.14713583 - Accuracy: 0.67400000 - Validation Accuracy: 0.49520000 - alpha: 0.04431924\n",
            "Iteration: 00012/00100 - Batch: [44500~45000]/45000 - Cost: 1.13512338 - Accuracy: 0.66200000 - Validation Accuracy: 0.51300000 - alpha: 0.04431924\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1aKQBer2Uveu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def imshow(img, title_info):\n",
        "    img = img / 2 + 0.5 # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.title(title_info)\n",
        "\n",
        "result = (NN.predict(X_test))\n",
        "test_list = [5, 11, 16, 21]\n",
        "for i in test_list:\n",
        "    imshow(testset[i][0], 'Groud Truth: %s - Prediction: %s' %(classes[testset[i][1]], classes[result[i]])) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SPc7ogqiHCoa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "htcLt3M7jldD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c2S_wf_djoMA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 2"
      ]
    }
  ]
}